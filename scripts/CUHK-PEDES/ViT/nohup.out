/data1/ccq/multimodality/CUHK-PEDES/20221010_144901_sketch2_add-fusion_original-twofocal-1-2-fusion-itcloss is not exists, create given directory
2022-10-10 14:49:01,008 CLIP2ReID INFO: Using 1 GPUs
2022-10-10 14:49:01,008 CLIP2ReID INFO: Namespace(MCM=False
 MCQ=False
 MCQMLM=False
 MLM=False
 MSM=False
 MSMMLM=False
 al=1.0
 alpha=0.9
 batch_size=64
 beta=0.999
 bias_lr_factor=2.0
 cmm_loss_weight=1.0
 cmt_depth=4
 dataset_name='CUHK-PEDES'
 distributed=False
 eval_period=1
 focal_three_fusion_loss=False
 focal_three_fusion_loss2=True
 focal_three_fusion_loss3=False
 focal_three_fusion_loss4=False
 focal_three_fusion_loss5=False
 focal_three_fusion_loss6=False
 focalthree_four_fusion_loss=False
 focalthree_fusion_loss=False
 four_fusion_loss=False
 fusion_way='add'
 ga=2.0
 gamma=0.1
 id_loss_weight=1.0
 img_aug=True
 img_size=(384
 128)
 klp=1.0
 label_mix=False
 learnable_loss_weight=False
 local_rank=0
 log_period=100
 loss_names='itc'
 lr=1e-05
 lr_factor=5.0
 lrscheduler='cosine'
 masked_token_rate=0.8
 masked_token_unchanged_rate=0.1
 mcm_loss_weight=1.0
 mcq_loss_weight=1.0
 milestones=(20
 50)
 mlm_loss_weight=1.0
 momentum=0.9
 name='sketch2_add-fusion_original-twofocal-1-2-fusion-itcloss'
 nlp_aug=False
 num_colors=60
 num_epoch=60
 num_instance=4
 num_workers=8
 only_fusion_loss=False
 only_sketch=False
 only_text=False
 optimizer='Adam'
 output_dir='/data1/ccq/multimodality/CUHK-PEDES/20221010_144901_sketch2_add-fusion_original-twofocal-1-2-fusion-itcloss'
 pa=0.1
 power=0.9
 pretrain_choice='ViT-B/16'
 resume=False
 resume_ckpt_file=''
 root_dir='/data0/data_ccq/CUHK-PEDES/'
 sampler='random'
 stride_size=16
 target_lr=0.0
 temperature=0.07
 test_batch_size=512
 test_setting=0
 text_length=77
 training=True
 use_imageid=False
 val_dataset='test'
 vocab_size=49408
 warmup_epochs=5
 warmup_factor=0.1
 warmup_method='linear'
 weight_decay=4e-05
 weight_decay_bias=0.0)
2022-10-10 14:49:01,008 CLIP2ReID INFO: Training only sketch False
2022-10-10 14:49:01,008 CLIP2ReID INFO: Using only text False
2022-10-10 14:49:01,008 CLIP2ReID INFO: Using add fusion method
2022-10-10 14:49:01,752 CLIP2ReID.dataset INFO: => CUHK-PEDES Images and Captions are loaded
2022-10-10 14:49:01,752 CLIP2ReID.dataset INFO: CUHKPEDES Dataset statistics:
2022-10-10 14:49:01,753 CLIP2ReID.dataset INFO: 
+--------+-------+--------+----------+
| subset |  ids  | images | captions |
+--------+-------+--------+----------+
| train  | 11003 | 34054  |  68126   |
|  test  |  1000 |  3074  |   6156   |
|  val   |  1000 |  3078  |   6158   |
+--------+-------+--------+----------+
2022-10-10 14:49:01,844 CLIP2ReID.dataset INFO: using random sampler
Training Model with ['itc'] tasks
2022-10-10 14:49:03,605 CLIP2ReID.model INFO: Load pretrained ViT-B/16 CLIP model with model config: {'embed_dim': 512, 'image_resolution': (384, 128), 'vision_layers': 12, 'vision_width': 768, 'vision_patch_size': 16, 'context_length': 77, 'vocab_size': 49408, 'transformer_width': 512, 'transformer_heads': 8, 'transformer_layers': 12, 'stride_size': 16}
Resized position embedding from size:torch.Size([1, 197, 768]) to size: torch.Size([1, 193, 768]) with height:24 width: 8
Using 5.0 times learning rate for random init module 
2022-10-10 14:49:07,065 CLIP2ReID.train INFO: start training
