2023-03-20 20:33:13,955 CLIP2ReID INFO: {'MCM': False, 'MCQ': False, 'MCQMLM': False, 'MLM': False, 'MSM': False, 'MSMMLM': False, 'al': 1.0, 'alpha': 0.9, 'batch_size': 64, 'beta': 0.999, 'bias_lr_factor': 2.0, 'cmm_loss_weight': 1.0, 'cmt_depth': 4, 'dataset_name': 'RSTPReid', 'distributed': False, 'eval_period': 1, 'focal_three_fusion_loss': False, 'focal_three_fusion_loss2': False, 'focal_three_fusion_loss3': False, 'focal_three_fusion_loss4': False, 'focal_three_fusion_loss5': False, 'focal_three_fusion_loss6': False, 'focalthree_four_fusion_loss': False, 'focalthree_fusion_loss': False, 'four_fusion_loss': False, 'fusion_way': 'add', 'ga': 3.5, 'gamma': 0.1, 'id_loss_weight': 1.0, 'img_aug': True, 'img_size': [384, 128], 'klp': 0.5, 'label_mix': False, 'learnable_loss_weight': False, 'local_rank': 0, 'log_period': 100, 'loss_names': 'itc', 'lr': 1e-05, 'lr_factor': 5.0, 'lrscheduler': 'cosine', 'masked_token_rate': 0.8, 'masked_token_unchanged_rate': 0.1, 'mcm_loss_weight': 1.0, 'mcq_loss_weight': 1.0, 'milestones': [20, 50], 'mlm_loss_weight': 1.0, 'momentum': 0.9, 'name': 'sketch2_text_itcloss', 'nlp_aug': False, 'num_colors': 60, 'num_epoch': 60, 'num_instance': 4, 'num_workers': 8, 'only_fusion_loss': False, 'only_sketch': False, 'only_text': True, 'optimizer': 'Adam', 'output_dir': '/data1/ccq/multimodality-RSTPReid/RSTPReid/20221017_142845_sketch2_text_itcloss', 'pa': 0.1, 'power': 0.9, 'pretrain_choice': 'ViT-B/16', 'resume': False, 'resume_ckpt_file': '', 'root_dir': '/data0/data_ccq/RSTPReid/', 'sampler': 'random', 'stride_size': 16, 'target_lr': 0.0, 'temperature': 0.07, 'test_batch_size': 512, 'test_setting': 0, 'text_length': 77, 'training': False, 'use_imageid': False, 'val_dataset': 'test', 'vocab_size': 49408, 'warmup_epochs': 5, 'warmup_factor': 0.1, 'warmup_method': 'linear', 'weight_decay': 4e-05, 'weight_decay_bias': 0.0}
2023-03-20 20:33:14,308 CLIP2ReID.dataset INFO: => RSTPReid Images and Captions are loaded
2023-03-20 20:33:14,308 CLIP2ReID.dataset INFO: RSTPReid Dataset statistics:
2023-03-20 20:33:14,309 CLIP2ReID.dataset INFO: 
+--------+------+--------+----------+
| subset | ids  | images | captions |
+--------+------+--------+----------+
| train  | 3701 | 18505  |  37010   |
|  test  | 200  |  1000  |   2000   |
|  val   | 200  |  1000  |   2000   |
+--------+------+--------+----------+
Training Model with ['itc'] tasks
2023-03-20 20:33:16,508 CLIP2ReID.model INFO: Load pretrained ViT-B/16 CLIP model with model config: {'embed_dim': 512, 'image_resolution': [384, 128], 'vision_layers': 12, 'vision_width': 768, 'vision_patch_size': 16, 'context_length': 77, 'vocab_size': 49408, 'transformer_width': 512, 'transformer_heads': 8, 'transformer_layers': 12, 'stride_size': 16}
Resized position embedding from size:torch.Size([1, 197, 768]) to size: torch.Size([1, 193, 768]) with height:24 width: 8
2023-03-20 20:33:24,616 CLIP2ReID.test INFO: Enter inferencing
2023-03-20 20:33:54,630 CLIP2ReID.eval INFO: 
+--------------------+-----------+------+-----------+---------+---------+
|        task        |     R1    |  R5  |    R10    |   mAP   |   mINP  |
+--------------------+-----------+------+-----------+---------+---------+
|    t2i-text_RGB    | 47.600002 | 68.5 | 77.100006 | 40.5077 | 23.8511 |
|   t2i-sketch_RGB   | 48.899998 | 67.4 |    76.0   | 31.1434 | 12.4252 |
| t2i-textsketch_RGB |   76.75   | 92.6 |   96.65   | 53.9641 | 26.8795 |
+--------------------+-----------+------+-----------+---------+---------+
